<!DOCTYPE html>
<html lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">


<link rel="stylesheet" href="/blog/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jasonguojz.github.io","root":"/blog/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="利用蒙特卡洛dropout来对认知不确定性进行建模，损失函数进行 MAP推断 从而对偶然不确定性进行建模">
<meta property="og:type" content="article">
<meta property="og:title" content="What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision?">
<meta property="og:url" content="https://jasonguojz.github.io/blog/2020/07/28/What%20Uncertainties%20Do%20We%20Need%20in%20Bayesian%20DeepLearning%20for%20Computer%20Vision/index.html">
<meta property="og:site_name" content="Guo Jiazhen&#39;s Blog">
<meta property="og:description" content="利用蒙特卡洛dropout来对认知不确定性进行建模，损失函数进行 MAP推断 从而对偶然不确定性进行建模">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805225714888.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805225410026.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805201612316.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805224525673.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805224759550.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805225626828.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805234050251.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805225859353.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805231547566.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200806113531761.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200806114448947.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200806195558780.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200806195540457.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200806201357626.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200806204638776.png">
<meta property="article:published_time" content="2020-07-27T16:00:00.000Z">
<meta property="article:modified_time" content="2020-08-26T03:12:09.351Z">
<meta property="article:tag" content="异方差不确定性">
<meta property="article:tag" content="CV">
<meta property="article:tag" content="aleatory and epistemic">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805225714888.png">

<link rel="canonical" href="https://jasonguojz.github.io/blog/2020/07/28/What%20Uncertainties%20Do%20We%20Need%20in%20Bayesian%20DeepLearning%20for%20Computer%20Vision/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision? | Guo Jiazhen's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Guo Jiazhen's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/blog/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/blog/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags<span class="badge">79</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/blog/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories<span class="badge">30</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/blog/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives<span class="badge">29</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonguojz.github.io/blog/2020/07/28/What%20Uncertainties%20Do%20We%20Need%20in%20Bayesian%20DeepLearning%20for%20Computer%20Vision/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="谦卑">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guo Jiazhen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision?
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-07-28 00:00:00" itemprop="dateCreated datePublished" datetime="2020-07-28T00:00:00+08:00">2020-07-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-26 11:12:09" itemprop="dateModified" datetime="2020-08-26T11:12:09+08:00">2020-08-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Uncertainty-in-deep-learning/" itemprop="url" rel="index"><span itemprop="name">Uncertainty in deep learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Dropout-Uncertainty/" itemprop="url" rel="index"><span itemprop="name">Dropout Uncertainty</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Computer-Vision/" itemprop="url" rel="index"><span itemprop="name">Computer Vision</span></a>
                </span>
            </span>

          
            <span id="/blog/2020/07/28/What%20Uncertainties%20Do%20We%20Need%20in%20Bayesian%20DeepLearning%20for%20Computer%20Vision/" class="post-meta-item leancloud_visitors" data-flag-title="What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision?" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/blog/2020/07/28/What%20Uncertainties%20Do%20We%20Need%20in%20Bayesian%20DeepLearning%20for%20Computer%20Vision/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/blog/2020/07/28/What%20Uncertainties%20Do%20We%20Need%20in%20Bayesian%20DeepLearning%20for%20Computer%20Vision/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>7.9k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>7 mins.</span>
            </span>
            <div class="post-description"><div align=center>利用蒙特卡洛dropout来对认知不确定性进行建模，损失函数进行 MAP推断 从而对偶然不确定性进行建模</div></div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <a id="more"></a>
<h3 id="『What-Uncertainties-Do-We-Need-in-Bayesian-DeepLearning-for-Computer-Vision-』阅读笔记"><a href="#『What-Uncertainties-Do-We-Need-in-Bayesian-DeepLearning-for-Computer-Vision-』阅读笔记" class="headerlink" title="『What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision?』阅读笔记"></a>『What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision?』阅读笔记</h3><h3 id="专有名词"><a href="#专有名词" class="headerlink" title="专有名词"></a>专有名词</h3><p>逐像素语义分割 <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41997920/article/details/96479243">综述解析</a></p>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>可以建模的不确定性有两种主要类型：偶然事件不确定性（Aleatoric Uncertainty）捕获观测中固有的噪声(不可约)。另一方面，模型的不确定性说明了模型中的认知不确定性Epistemic Uncertainty-如果有足够的数据，就可以解释不确定性。传统上，很难对计算机视觉中的认知不确定性进行建模，但是现在有了新的<strong>贝叶斯深度学习工具</strong>，这是可能的。我们研究了在<strong>视觉任务</strong>的贝叶斯深度学习模型中<strong>对认知不确定性（Epistemic Uncertainty）与偶然事件不确定性（Aleatoric Uncertainty）建模</strong>的好处。为此，我们提出了一种贝叶斯深度学习框架，将输入依赖input-dependent的偶然不确定性 与 认知不确定性相结合。我们在具有<strong>逐像素语义分割和深度回归任务的框架下研究模型</strong>。此外，我们明确的不确定性公式 explicit uncertainty formulation  产生这些任务的新损失函数，这可以解释为学习衰减 learned attenuation。这使得损失对噪声数据的鲁棒性更高，还为分割和深度回归基准提供了最新的最新结果。</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h3><p>很多机器学习算法可以很好地将高维空间的数据映射成低维数组，但很少考虑这些映射的准确率，从而导致很多灾难性的后果。</p>
<p><strong>计算机视觉</strong>应用程序中的<strong>不确定性量化</strong>可以大致分为诸如<strong>regression</strong> settings such as <strong>depth regression</strong>, and <strong>classification</strong> settings such as <strong>semantic segmentation</strong>。在计算机视觉的这种设置中，对不确定性进行建模的现有方法包括<strong>粒子滤波和条件随机场</strong> particle filtering andconditional random fields。大多数深度学习模型都无法表示不确定性。深度学习在现有的回归设置中不能表示不确定性，深度学习分类模型通常会提供归一化的分数向量，(softmax输出向量)无法捕获模型不确定性。对于这两种设置，都可以使用<strong>贝叶斯深度学习方法</strong>捕获不确定性，这为理解深度学习模型的不确定性提供了实用的框架[6]。</p>
<p>在贝叶斯建模中，可以对模型进行建模的不确定性有两种主要类型[7]。Aleatoric uncertainty，这可能是传感器噪声或运动噪声，从而导致不确定性，即使要<strong>收集更多的数据也无法降低不确定性</strong>。另一方面，epistemic uncertainty 是模型参数中的不确定性——这抓住了我们对哪个模型产生了对我们收集的数据的无知。如果<strong>有足够的数据，就可以解释这种不确定性</strong>，它通常被称为模型不确定性。Aleatoric uncertainty可以进一步分为同方差不确定性：对不同输入保持不变的不确定性和异方差不确定性；异方差不确定性取决于模型的输入，其中一些输入可能比其他输入具有更大的噪声输出。<strong>异方差不确定性对于计算机视觉应用尤其重要</strong>。例如，对于深度回归，具有强烈消失线的高度纹理化的输入图像预期会产生不自信的预测，而像无明显特征的墙 作为输入图像，预期会有非常高的不确定性</p>
<p>在本文中，我们观察到在许多<strong>大数据体系中(比如那些与图像数据有关的深度学习的体系)，建模偶然不确定性aleatoric uncertainty是最有效的</strong>——这类不确定性是不能被解释的。这是与认知的不确定性相比较，而认知的不确定性通常可以用机器视觉中大量的数据来解释。我们进一步证明建模 aleatoric uncertainty 是有代价的。数据外的例子，可以用epistemic uncertainty识别，不能单独用 aleatoric uncertainty 识别。</p>
<p>为此，我们提出了一个统一的贝叶斯深度学习框架，它允许我们从输入数据 映射到 aleatoric uncertainty，并将这些与epistemic uncertainty的估计组合在一起。我们推导了回归和分类应用的框架，并给出了逐像素深度回归和语义分割任务的结果(示例见图1和补充视频)。我们展示了如何在回归任务中建模aleatoric uncertainty，并可用于学习loss attenuation，并发展一个互补的方法为分类情况。这表明了我们处理困难和大规模任务的方法的有效性</p>
<h4 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h4><ol>
<li>capture an accurate understanding of aleatoric and epistemic uncertainties, in particular with a novel approach for classification</li>
<li>通过减少噪声数据的影响，在非贝叶斯基线上提高模型性能1 - 3%，从明确表示偶然不确定性  explicitly  representing aleatoric uncertainty 获得的隐含衰减  implied  attenuation</li>
<li>我们通过刻画每个不确定性的性质并比较模型的性能和推理时间来研究建模  aleatoric or epistemic uncertainty  之间的权衡</li>
</ol>
<h3 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2    Related Work"></a>2    Related Work</h3><p><strong>现有的贝叶斯深度学习方法要么只捕捉认知不确定性，要么只捕捉偶然不确定性[6]。</strong>这些不确定性分别被表示为模型参数的概率分布或模型输出的概率分布。认知的不确定性是通过在模型的权重上放置一个先验分布来建模的，然后尝试捕捉给定一些数据的权重变化的多少。另一方面，偶然不确定性是通过 by placing a distribution over the output of the mode来建模的。<strong>例如，在回归中，我们的输出可能被建模为带有高斯随机噪声的</strong>。在这种情况下，我们感兴趣的是学习噪声的方差作为不同输入的函数(这种噪声也可以为所有数据点建模为一个常数值，但这没有意义)。这些不确定性，在贝叶斯深入的背景下，将在本节更详细地解释</p>
<h4 id="2-1-Epistemic-Uncertainty-in-Bayesian-Deep-Learning认知不确定性建模"><a href="#2-1-Epistemic-Uncertainty-in-Bayesian-Deep-Learning认知不确定性建模" class="headerlink" title="2.1    Epistemic Uncertainty in Bayesian Deep Learning认知不确定性建模"></a>2.1    Epistemic Uncertainty in Bayesian Deep Learning认知不确定性建模</h4><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39779106/article/details/78968982">Reference—Xieyuanli_Chen 的 cnblog</a></p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805225714888.png" alt="image-20200805225714888" style="zoom:80%;" /></p>
<p>为模型参数分布建模，$P(W|X,Y)$，通常通过网络模型参数的概率分布来研究认知不确定性，首先通过将先验分布置于模型的权重上进行建模，然后尝试计算这些权重随着数据变化的规律</p>
<p>在分类问题中，预测不确定性可以利用蒙特卡洛积分来近似</p>
<p>在回归问题中，这一认知不确定性可以通过预测方差来进行计算</p>
<h4 id="2-2-Heteroscedastic-Aleatoric-Uncertainty异方差偶然不确定性计算"><a href="#2-2-Heteroscedastic-Aleatoric-Uncertainty异方差偶然不确定性计算" class="headerlink" title="2.2    Heteroscedastic Aleatoric Uncertainty异方差偶然不确定性计算"></a>2.2    Heteroscedastic Aleatoric Uncertainty异方差偶然不确定性计算</h4><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39779106/article/details/78968982">Reference—cnblog</a></p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805225410026.png" alt="image-20200805225410026" style="zoom:80%;" /></p>
<p>MAP推断</p>
<p>研究网络输出来研究偶然不确定性，通常是对模型的输出进行拟合。例如在回归问题中，我们可以把输出建模为随着高斯随机噪声而衰减</p>
<p>计算回归问题中的偶然不确定性，我们需要对观测噪声参数 $\sigma$ 进行调整，计算异方差偶然不确定性，认为噪声的方差是不同输入的函数。</p>
<h3 id="3-Combining-Aleatoric-and-Epistemic-Uncertainty-in-One-Model"><a href="#3-Combining-Aleatoric-and-Epistemic-Uncertainty-in-One-Model" class="headerlink" title="3    Combining Aleatoric and Epistemic Uncertainty in One Model"></a>3    Combining Aleatoric and Epistemic Uncertainty in One Model</h3><p>我们开发的模型将允许我们研究单独建模偶然不确定性、单独建模认知不确定性或在单个模型中同时建模这两个不确定性的影响。接着观察到回归任务中的偶然不确定性可以解释为学习损失衰减loss attenuation，这使得损失对噪声数据更加稳健。我们将<strong>异方差回归的思想扩展到分类任</strong>务。这也使我们能够了解分类任务的损耗衰减loss attenuation</p>
<h4 id="3-1-Combining-Heteroscedastic-Aleatoric-Uncertainty-and-Epistemic-Uncertainty"><a href="#3-1-Combining-Heteroscedastic-Aleatoric-Uncertainty-and-Epistemic-Uncertainty" class="headerlink" title="3.1    Combining Heteroscedastic Aleatoric Uncertainty and Epistemic Uncertainty"></a>3.1    Combining Heteroscedastic Aleatoric Uncertainty and Epistemic Uncertainty</h4><p>为了同时捕捉认知和任意的不确定性，将§2.2中的异方差神经网络转换为贝叶斯神经网络，方法是将分布置于其权重之上。</p>
<p>我们需要推断一个BNN模型 $f$ 的后验分布，它将输入图像 x 映射到非随机输出<img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805201612316.png" alt="image-20200805201612316" style="zoom:80%;" />，以及输出方差 $σ^2$ 给出的偶然不确定度的度量。我们使用 §2.1 中的公式，用一个 <strong>dropout变分分布</strong> 来近似BNN模型 $f$ 的后验分布。</p>
<p>结合BNN网络的随机输出分布的均值和方差：<img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805224525673.png" alt="image-20200805224525673" style="zoom:80%;" /></p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805224759550.png" alt="image-20200805224759550" style="zoom:80%;" /></p>
<p>公式(第一张按照#2.1公式，第二张按照#3公式，两者一致，体现在权重表示为分布形式)：</p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805225626828.png" alt="image-20200805225626828" style="zoom:80%;" /></p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805234050251.png" alt="image-20200805234050251" style="zoom:80%;" /></p>
<p>建模偶然不确定性，第二项正则项防止第一项中的方差取极大：</p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805225859353.png" alt="image-20200805225859353" style="zoom:80%;" /></p>
<p>加个对数避免了第一项除以零的数值不稳定性，</p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200805231547566.png" alt="image-20200805231547566"></p>
<p>combine 体现在将 #2.2 中 $f$ 固定的权重改成 概率分布，噪声方差的计算不同</p>
<h4 id="3-2-Heteroscedastic-Uncertainty-as-Learned-Loss-Attenuation"><a href="#3-2-Heteroscedastic-Uncertainty-as-Learned-Loss-Attenuation" class="headerlink" title="3.2    Heteroscedastic Uncertainty as Learned Loss Attenuation"></a>3.2    Heteroscedastic Uncertainty as Learned Loss Attenuation</h4><p>我们发现允许网络预测不确定性使得其可以通过 $exp(−s_i)$ 有效地减少损失残差(不确定性大时，会迫使第一项中的残差项减小)。这一行为与一个智能鲁棒的回归函数是类似的，它允许网络适应残差的权重，甚至允许网络学习<strong>减弱 错误标签 的影响</strong> (错误标签，高不确定性为小权重)，这使得模型对噪声的鲁棒性增强：针对预测出高不确定性的输入，模型将对损失函数产生较小的影响。</p>
<p>这一模型不鼓励对所有的输入产生高不确定性——通过 $logσ^2$ 的形式实现 (<strong>正则项</strong>) ——因为会使得模型忽略这些输入。因此当高不确定性输入很多时将会对模型进行惩罚——即允许模型学会忽略数据，但会对其做出惩罚。这一模型同样不鼓励预测出不确定性低但残差高的结果，因为低 $\sigma^2$ 值会过于扩大残差的影响，同样会对模型进行惩罚。这种学习衰减不是一种特殊设计的结构，而是模型概率解释的结果——通过加正则项，以及建模出两个因素间的trade off来实现 Loss Attenuation</p>
<h4 id="3-3-Heteroscedastic-Uncertainty-in-Classification-Tasks"><a href="#3-3-Heteroscedastic-Uncertainty-in-Classification-Tasks" class="headerlink" title="3.3    Heteroscedastic Uncertainty in Classification Tasks"></a>3.3    Heteroscedastic Uncertainty in Classification Tasks</h4><p>分类中的异方差神经网络是一种特殊的分类模型，因为从技术上讲，任何分类任务都具有输入依赖的不确定性</p>
<ol>
<li><p>在分类问题中，NN将会对每一个像素 $i$ 预测一个一元数组 $f_i$，当经过一个softmax运算后将形成一个概率数组 $p_i$</p>
</li>
<li><p>We change the model by placing a Gaussian distribution over the unaries vector $f_i$ 把对每个 pixel $i$ 的unary vector $f_i$ 用一个 高斯分布随机变量替代：</p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200806113531761.png" alt="image-20200806113531761" style="zoom:80%;" /></p>
<p>每个 $f_i$ indexed by i 都被注入了 方差为 $\sigma^W_i$ 的高斯噪声，即 Heteroscedastic Uncertainty </p>
</li>
<li><p>新模型的损失函数：对数似然</p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200806114448947.png" alt="image-20200806114448947" style="zoom:80%;" /></p>
<p>求这个期望的时候，我们想将这个高斯分布解析地积分出来，但是没有解析解是已知的。我们采用<strong>蒙特卡洛积分</strong>来近似目标，sample unaries through the softmax function.。我们注意到这个操作是非常快的，因为我们只执行一次计算（将输入经过一次模型便可计算得到对数值）。我们只需要对softmax的输出进行抽样，这只是整个网络计算整体的一部分，因此并不会增加测试时的计算时间。因此数值稳定的损失函数如下所示，</p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200806195558780.png" alt="image-20200806195558780" style="zoom:80%;" /></p>
<p>这个损失函数形式变化，参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/81170602">nameoverflow——Bayesian Neural Networks：贝叶斯神经网络</a>，使得可以应用现有的优化方式</p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200806195540457.png" alt="image-20200806195540457" style="zoom:80%;" /></p>
</li>
</ol>
<h3 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4    Experiments"></a>4    Experiments</h3><p>本文在像素级的深度回归和语义分割问题上对所提出的模型进行了测试。为了展示本文所提出的可以学习的损失衰减的鲁棒性（对不确定性建模的好处之一），我们在CamVid, Make3D和NYUv2 Depth数据集上进行了测试，并取得了目前最好的性能。在实验中，我们利用了DenseNet的框架（用于深度回归问题），并对其稍微进行了改进（在CamVid上进行测试比改进前的性能提高了0.2%）。在所有的实验中，我们将训练图像裁剪成224x224，batch的大小为4，然后利用全尺寸图进行精调，batch大小为1，采用RMS-Prop优化方法，学习率为0.001，权值衰减率为10−410−4。我们<strong>利用蒙特卡洛dropout来对认知不确定性进行建模</strong>，DenseNet框架中采用的dropout概率为 $p=0.2$ ，在每一个卷积层后使用，本文中我们使用50个蒙特卡洛dropout采样。我们利用<strong>上文提到的损失函数进行 MAP推断 从而对偶然不确定性进行建模</strong>。在实际实验过程中，我们采用的是拉普拉斯先验(L1)而不是高斯先验(L2)，因为其采用的L1距离描述残差比高斯采用的L2距离更适合视觉回归问题</p>
<h4 id="4-1-Semantic-Segmentation"><a href="#4-1-Semantic-Segmentation" class="headerlink" title="4.1    Semantic Segmentation"></a>4.1    Semantic Segmentation</h4><p>在此实验中，我们采用了CamVid和NYUv2数据集，其中CamVid是道路场景数据集包含367张训练图片以及233张测试图片，11个类别,实验结果如表一a所示，可以看出偶然不确定性对性能影响更大，结合两种不确定时系统性能最佳。NYUv2数据集是一个具有挑战的室内分类数据集，包含40中语义类别，实验结果如表一b所示。</p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200806201357626.png" alt="image-20200806201357626" style="zoom:80%;" /></p>
<p>mean intersection over union (IoU) score 显示：this application it is <strong>more important to model aleatoric uncertainty</strong>, suggesting that <strong>epistemic uncertainty can be mostly explained away in this large data setting.</strong></p>
<h4 id="4-2-Pixel-wise-Depth-Regression"><a href="#4-2-Pixel-wise-Depth-Regression" class="headerlink" title="4.2    Pixel-wise Depth Regression"></a>4.2    Pixel-wise Depth Regression</h4><p>在此实验中，我们采用了Make3D和NYUv2 Depth数据集，实验结果如表二所示，结果表明<strong>偶然不确定在此类问题中发挥了很大作用</strong>，如图五、图六所示，在图像深度较深，反射表面以及遮挡边界处large depths, reflective surfaces and occlusion boundaries in the image的偶然不确定性值很大，这些地方往往是单目深度算法容易失败的地方。<strong>反观由于数据量太少，认知不确定很难发挥大作用。</strong>总的来说，我们通过直接学习系统噪声和复杂概念的衰减从而提高了非贝叶斯网络的性能，例如我们观察到遥远物体和物体和遮挡边界的偶然不确定性是比较高的。</p>
<h3 id="5-Analysis-What-Do-Aleatoric-and-Epistemic-Uncertainties-Capture"><a href="#5-Analysis-What-Do-Aleatoric-and-Epistemic-Uncertainties-Capture" class="headerlink" title="5    Analysis: What Do Aleatoric and Epistemic Uncertainties Capture?"></a>5    Analysis: What Do Aleatoric and Epistemic Uncertainties Capture?</h3><p>在这一节中，我们希望研究建模偶然和认知的不确定性的有效性。特别地，我们希望量化这些不确定度测量的性能，并分析它们捕获了什么</p>
<h4 id="5-1-Quality-of-Uncertainty-Metric"><a href="#5-1-Quality-of-Uncertainty-Metric" class="headerlink" title="5.1    Quality of Uncertainty Metric"></a>5.1    Quality of Uncertainty Metric</h4><p>在图二中我们给出了回归问题和分类问题的PR曲线，PR曲线说明了我们的模型性能可以通过消除不确定性大于方差阈值的像素来提高。这表示了不确定性的两种行为，一是不确定性测量与精度是相关的，因为所有曲线都是严格递减函数，当模型有更多不确定的点时，精度会降低；二是两种不确定性的曲线是相似的，在没有其他不确定性的情况下，每个不确定性对像素置信度的排序与其他不确定性相似，即使当只有一个不确定性能被建模时，它会在一定程度上弥补另一不确定性</p>
<p>在图三中我们用我们模型在测试集上的校准图分析不确定性度量。对于分类问题而言，我们通过将我们模型预测的概率离散化成一些数，然后画出正确预测的标注的频率对应的数，不确定性质量越高的预测应该与$y=x$更加接近。对于回归问题而言，我们可以通过比较预测分布的变化阈值内的残差频率来形成校准图we can form calibration plots by comparing the frequency of residuals lying within varying thresholds of the predicted distribution. </p>
<h4 id="5-2-Uncertainty-with-Distance-from-Training-Data"><a href="#5-2-Uncertainty-with-Distance-from-Training-Data" class="headerlink" title="5.2    Uncertainty with Distance from Training Data"></a>5.2    Uncertainty with Distance from Training Data</h4><ol>
<li>偶然不确定性无法通过更多数据解释。</li>
<li>偶然不确定性也不会因为与训练集不同的样本而增加，而认知不确定性会</li>
</ol>
<p>在表三中我们给出了在子集不断增加的数据上训练模型的精度与不确定性，结果表明<strong>认知不确定性将随训练集增大而减小，结果同时表明偶然不确定性保持相对稳定，不能被更多数据解释。利用不同的数据集进行测试时认知不确定性会稍微增加。</strong></p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/image-20200806204638776.png" alt="image-20200806204638776" style="zoom:80%;" /></p>
<h4 id="5-3-Real-Time-Application"><a href="#5-3-Real-Time-Application" class="headerlink" title="5.3    Real-Time Application"></a>5.3    Real-Time Application</h4><p>偶然不确定性增加的时间可以忽略不计，认知不确定性<strong>蒙特卡洛采样是比较耗时的</strong>   ResNet只有最后几层有dropout需要采样，而denseNet require the entire architecture to be sample，因为CPU的显存限制</p>
<h3 id="6-Conclusions"><a href="#6-Conclusions" class="headerlink" title="6   Conclusions"></a>6   Conclusions</h3><p>对以下情形计算偶然不确定性是比较重要的：</p>
<ol>
<li>具有<strong>大量数据</strong>的情况，这种情况下认知不确定性是可以被解释的。</li>
<li><strong>实时系统</strong>，偶然不确定性不会影响实时性。</li>
</ol>
<p>对以下请性计算认知不确定性是比较重要的：</p>
<ol>
<li>对<strong>安全性要求较高</strong>的应用，对uncertainty比较敏感的应用，因为认知性能可以识别出当前场景与训练集是否一致。</li>
<li>小数据集情况，training data is sparse时</li>
</ol>
<hr>
<h4 id="偶然事件不确定性（Aleatoric-Uncertainty）和-认知不确定性（Epistemic-Uncertainty）"><a href="#偶然事件不确定性（Aleatoric-Uncertainty）和-认知不确定性（Epistemic-Uncertainty）" class="headerlink" title="偶然事件不确定性（Aleatoric Uncertainty）和 认知不确定性（Epistemic Uncertainty）"></a><em>偶然事件不确定性（Aleatoric Uncertainty）</em>和 <em>认知不确定性（Epistemic Uncertainty）</em></h4><h4 id="reference—深度学习中的两种不确定性"><a href="#reference—深度学习中的两种不确定性" class="headerlink" title="reference—深度学习中的两种不确定性"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/56986840">reference—深度学习中的两种不确定性</a></h4><p>传统深度学习算法几乎只能给出一个特定的结果，而不能给出模型自己对结果的置信度。当输入不在训练集出现过的样本时，softmax输出概率不太可能是在标签集上的平均值如(0.5,0.5)<a target="_blank" rel="noopener" href="http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html">原因</a></p>
<p>BNN (Bayesian Neural Network)。BNN的原理大体上是，我们网络中每个参数的weight将不再是一个特定的数字，取而代之的是一个先验分布。这样我们train出来的网络将不再是一个函数，而是一个函数的分布<a target="_blank" rel="noopener" href="https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd">BNN详细</a></p>
<h5 id="1-偶然不确定性"><a href="#1-偶然不确定性" class="headerlink" title="1    偶然不确定性"></a>1    偶然不确定性</h5><p>数据本来就存在误差。<strong>数据集里这样的bias越大</strong>，我们的偶然不确定性就应该越大。(来自数据收集过程的<strong>不可约减的噪声</strong>,这个现象不能通过增加采样数据来削弱，解决这个问题的方法一般是提升数据采集时候的稳定性，或者提升衡量指标的精度以囊括各类客观影响因素)</p>
<p>可以进一步分为同方差不确定性（Task-dependant or Homoscedastic uncertainty）和异方差不确定性（Data-dependant or Heteroscedastic uncertainty）</p>
<ul>
<li>异方差不确定性，取决于输入数据，并预测为模型输出。其中一些输入可能具有比其他输入更多的噪声输出。异方差的不确定性尤为重要，可以防止模型输出非常自信的决策</li>
<li>同方差不确定性，不取决于输入数据。它不是模型输出，而是一个对所有输入数据保持不变并且在不同任务之间变化的数量。因此，它可以被描述为任务相关的不确定性</li>
</ul>
<h5 id="2-认知不确定性"><a href="#2-认知不确定性" class="headerlink" title="2    认知不确定性"></a>2    认知不确定性</h5><p>认知不确定性测量的，是我们的input data是否存在于已经见过的数据的分布之中。(对真实模型的无知，模型自身对输入数据的估计可能因为训练不佳、训练数据不够等原因而不准确，与某一单独的数据无关。可以通过有针对性的调整（增加训练数据等方式）来缓解甚至解决的)</p>
<h4 id="两种不确定性的量化"><a href="#两种不确定性的量化" class="headerlink" title="两种不确定性的量化"></a>两种不确定性的量化</h4><p>对于回归问题</p>
<h5 id="1-认知不确定性的量化"><a href="#1-认知不确定性的量化" class="headerlink" title="1    认知不确定性的量化"></a>1    认知不确定性的量化</h5><p>估计数据集的真实分布 $P(D)$  $D$ 为数据集，$W$ 为权重</p>
<p>蒙特卡洛方法对网络参数的后验概率 $P(W|D)$ 进行估计，后验概率 $P(W|D)$ (我们就可以知道 $D$ 到底在不在我们已经学习的分布中，从而获得认知不确定性)</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/geo-will/p/10491447.html">reference—贝叶斯深度学习-概述</a></p>
<p><a target="_blank" rel="noopener" href="https://aijishu.com/a/1060000000089656">R TALK | 旷视危夷晨：不确定性学习在视觉识别中的应用</a></p>
<hr>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>guo jiazhen
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://jasonguojz.github.io/blog/2020/07/28/What%20Uncertainties%20Do%20We%20Need%20in%20Bayesian%20DeepLearning%20for%20Computer%20Vision/" title="What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision?">https://jasonguojz.github.io/blog/2020/07/28/What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/blog/tags/%E5%BC%82%E6%96%B9%E5%B7%AE%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7/" rel="tag"># 异方差不确定性</a>
              <a href="/blog/tags/CV/" rel="tag"># CV</a>
              <a href="/blog/tags/aleatory-and-epistemic/" rel="tag"># aleatory and epistemic</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/blog/2020/07/27/Aleatory%20or%20epistemic%20Does%20it%20matter/" rel="prev" title="Aleatory or epistemic? Does it matter?">
      <i class="fa fa-chevron-left"></i> Aleatory or epistemic? Does it matter?
    </a></div>
      <div class="post-nav-item">
    <a href="/blog/2020/07/29/Dropout%20as%20a%20Bayesian%20Approximation%20Representing%20Model%20Uncertainty%20in%20Deep%20Learning/" rel="next" title="Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning">
      Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E3%80%8EWhat-Uncertainties-Do-We-Need-in-Bayesian-DeepLearning-for-Computer-Vision-%E3%80%8F%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0"><span class="nav-number">1.</span> <span class="nav-text">『What Uncertainties Do We Need in Bayesian DeepLearning for Computer Vision?』阅读笔记</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%93%E6%9C%89%E5%90%8D%E8%AF%8D"><span class="nav-number">2.</span> <span class="nav-text">专有名词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract"><span class="nav-number">3.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Introduction"><span class="nav-number">4.</span> <span class="nav-text">1    Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%A1%E7%8C%AE"><span class="nav-number">4.1.</span> <span class="nav-text">贡献</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Related-Work"><span class="nav-number">5.</span> <span class="nav-text">2    Related Work</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Epistemic-Uncertainty-in-Bayesian-Deep-Learning%E8%AE%A4%E7%9F%A5%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%BB%BA%E6%A8%A1"><span class="nav-number">5.1.</span> <span class="nav-text">2.1    Epistemic Uncertainty in Bayesian Deep Learning认知不确定性建模</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-Heteroscedastic-Aleatoric-Uncertainty%E5%BC%82%E6%96%B9%E5%B7%AE%E5%81%B6%E7%84%B6%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E8%AE%A1%E7%AE%97"><span class="nav-number">5.2.</span> <span class="nav-text">2.2    Heteroscedastic Aleatoric Uncertainty异方差偶然不确定性计算</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Combining-Aleatoric-and-Epistemic-Uncertainty-in-One-Model"><span class="nav-number">6.</span> <span class="nav-text">3    Combining Aleatoric and Epistemic Uncertainty in One Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Combining-Heteroscedastic-Aleatoric-Uncertainty-and-Epistemic-Uncertainty"><span class="nav-number">6.1.</span> <span class="nav-text">3.1    Combining Heteroscedastic Aleatoric Uncertainty and Epistemic Uncertainty</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Heteroscedastic-Uncertainty-as-Learned-Loss-Attenuation"><span class="nav-number">6.2.</span> <span class="nav-text">3.2    Heteroscedastic Uncertainty as Learned Loss Attenuation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-Heteroscedastic-Uncertainty-in-Classification-Tasks"><span class="nav-number">6.3.</span> <span class="nav-text">3.3    Heteroscedastic Uncertainty in Classification Tasks</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Experiments"><span class="nav-number">7.</span> <span class="nav-text">4    Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-Semantic-Segmentation"><span class="nav-number">7.1.</span> <span class="nav-text">4.1    Semantic Segmentation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-Pixel-wise-Depth-Regression"><span class="nav-number">7.2.</span> <span class="nav-text">4.2    Pixel-wise Depth Regression</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Analysis-What-Do-Aleatoric-and-Epistemic-Uncertainties-Capture"><span class="nav-number">8.</span> <span class="nav-text">5    Analysis: What Do Aleatoric and Epistemic Uncertainties Capture?</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-Quality-of-Uncertainty-Metric"><span class="nav-number">8.1.</span> <span class="nav-text">5.1    Quality of Uncertainty Metric</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-Uncertainty-with-Distance-from-Training-Data"><span class="nav-number">8.2.</span> <span class="nav-text">5.2    Uncertainty with Distance from Training Data</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-Real-Time-Application"><span class="nav-number">8.3.</span> <span class="nav-text">5.3    Real-Time Application</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Conclusions"><span class="nav-number">9.</span> <span class="nav-text">6   Conclusions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%81%B6%E7%84%B6%E4%BA%8B%E4%BB%B6%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%EF%BC%88Aleatoric-Uncertainty%EF%BC%89%E5%92%8C-%E8%AE%A4%E7%9F%A5%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%EF%BC%88Epistemic-Uncertainty%EF%BC%89"><span class="nav-number">9.1.</span> <span class="nav-text">偶然事件不确定性（Aleatoric Uncertainty）和 认知不确定性（Epistemic Uncertainty）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#reference%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%B8%A4%E7%A7%8D%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7"><span class="nav-number">9.2.</span> <span class="nav-text">reference—深度学习中的两种不确定性</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E5%81%B6%E7%84%B6%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7"><span class="nav-number">9.2.1.</span> <span class="nav-text">1    偶然不确定性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E8%AE%A4%E7%9F%A5%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7"><span class="nav-number">9.2.2.</span> <span class="nav-text">2    认知不确定性</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%A4%E7%A7%8D%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%9A%84%E9%87%8F%E5%8C%96"><span class="nav-number">9.3.</span> <span class="nav-text">两种不确定性的量化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E8%AE%A4%E7%9F%A5%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%9A%84%E9%87%8F%E5%8C%96"><span class="nav-number">9.3.1.</span> <span class="nav-text">1    认知不确定性的量化</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">谦卑</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/blog/archives/">
        
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/blog/categories/">
          
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/blog/tags/">
          
        <span class="site-state-item-count">79</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/JasonGuojz" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;JasonGuojz" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/blog/jzguovulcan@gmail.com" title="E-Mail → jzguovulcan@gmail.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="skype:1142346305?call|chat" title="Skype → skype:1142346305?call|chat" rel="noopener" target="_blank"><i class="fa fa-fw fa-skype"></i>Skype</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder"></span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="Symbols count total">195k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">2:58</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/blog/lib/anime.min.js"></script>
  <script src="/blog/lib/velocity/velocity.min.js"></script>
  <script src="/blog/lib/velocity/velocity.ui.min.js"></script>

<script src="/blog/js/utils.js"></script>

<script src="/blog/js/motion.js"></script>


<script src="/blog/js/schemes/muse.js"></script>


<script src="/blog/js/next-boot.js"></script>




  




  
<script src="/blog/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '2M2aiiBff85KzN3JvTFsCV2J-9Nh9j0Va',
      appKey     : '7n1fFOWyaLnTBNpNjRhYsF0W',
      placeholder: "Please write your comments here",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
