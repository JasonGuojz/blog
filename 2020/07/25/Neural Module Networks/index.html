<!DOCTYPE html>
<html lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.0.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">


<link rel="stylesheet" href="/blog/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jasonguojz.github.io","root":"/blog/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Neural Module Networks 用于 VQA">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Module Networks">
<meta property="og:url" content="https://jasonguojz.github.io/blog/2020/07/25/Neural%20Module%20Networks/index.html">
<meta property="og:site_name" content="Guo Jiazhen&#39;s Blog">
<meta property="og:description" content="Neural Module Networks 用于 VQA">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200724165527393.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725094320268.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725101355203.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725102011503.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725140429705.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725102258978.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725102646051.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725112729958.png">
<meta property="og:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725144242301.png">
<meta property="article:published_time" content="2020-07-24T16:00:00.000Z">
<meta property="article:modified_time" content="2020-08-26T03:11:53.142Z">
<meta property="article:tag" content="dependency parsing">
<meta property="article:tag" content="CLEVR dataset">
<meta property="article:tag" content="SHAPES dataset">
<meta property="article:tag" content="dynamic network structure">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200724165527393.png">

<link rel="canonical" href="https://jasonguojz.github.io/blog/2020/07/25/Neural%20Module%20Networks/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Neural Module Networks | Guo Jiazhen's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Guo Jiazhen's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/blog/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/blog/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags<span class="badge">79</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/blog/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories<span class="badge">30</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/blog/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives<span class="badge">29</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://jasonguojz.github.io/blog/2020/07/25/Neural%20Module%20Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="谦卑">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Guo Jiazhen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Neural Module Networks
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-07-25 00:00:00" itemprop="dateCreated datePublished" datetime="2020-07-25T00:00:00+08:00">2020-07-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-26 11:11:53" itemprop="dateModified" datetime="2020-08-26T11:11:53+08:00">2020-08-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Visual-Reasoning/" itemprop="url" rel="index"><span itemprop="name">Visual Reasoning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/VQA/" itemprop="url" rel="index"><span itemprop="name">VQA</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Neural-Module-Networks/" itemprop="url" rel="index"><span itemprop="name">Neural Module Networks</span></a>
                </span>
            </span>

          
            <span id="/blog/2020/07/25/Neural%20Module%20Networks/" class="post-meta-item leancloud_visitors" data-flag-title="Neural Module Networks" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/blog/2020/07/25/Neural%20Module%20Networks/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/blog/2020/07/25/Neural%20Module%20Networks/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>12k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>11 mins.</span>
            </span>
            <div class="post-description"><div align=center>Neural Module Networks 用于 VQA</div></div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <a id="more"></a>
<h3 id="『Neural-Module-Networks』阅读笔记"><a href="#『Neural-Module-Networks』阅读笔记" class="headerlink" title="『Neural Module Networks』阅读笔记"></a>『Neural Module Networks』阅读笔记</h3><ol>
<li>网络模块的基本组成，使得compositional 的 structure能覆盖大部分的问题</li>
<li>将自然语言问题 映射 到 布局 layouts，布局包括所用模块和模块间的连接关系，根据 layouts to assemble the final prediction networks。</li>
<li>自然语言问题通过<strong>依存句法分析和基本的词法处理</strong></li>
<li>因为动态的网络结构，some weights are updated much more frequently than others<strong>. 自适应学习率算法</strong>更好，但有人指出调参能力一般</li>
<li>Created <strong>SHAPES,</strong> a synthetic dataset that places such compositional phenomena at the forefront</li>
</ol>
<h3 id="code"><a href="#code" class="headerlink" title="code"></a><a target="_blank" rel="noopener" href="https://github.com/jacobandreas/nmn2">code</a></h3><h3 id="专有名词"><a href="#专有名词" class="headerlink" title="专有名词"></a>专有名词</h3><ol>
<li><p><strong>VQA: </strong>   Visual question answering </p>
</li>
<li><p><strong>modeling common sense</strong> knowledge </p>
</li>
<li><p><strong>dataset biases</strong></p>
</li>
<li><p><strong>grounding the question in the image</strong>——grounding task</p>
</li>
<li><p><strong>更普遍地是利用 集理论 set-theoretic approaches  之于 经典语义解析方法 classical semantic parsing 和 注意方法attentional approaches 之于计算机视觉 之间的自然相似性。</strong></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://bindog.github.io/blog/2018/02/10/model-explanation/">CAM 和 Grad-CAM</a></p>
</li>
<li><p><strong>lexical analysis——词法分析</strong> 是计算机科学中将字符序列转换为<strong>标记</strong>（token）序列的过程</p>
</li>
<li><p>adaptive per-weight learning rates</p>
</li>
<li><p>hard perceptual problems </p>
</li>
<li><p>monolithic network</p>
</li>
</ol>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p><strong>Visual question answering</strong> 本质上是<strong>组合的</strong>，——例如 where is the dog ？与 what color is the dog? where is the cat?  这些问题都<strong>有同样的语言学的子结构 substructure</strong> 。 本文旨在同时<strong>利用深层网络的表示能力  representational capacity 和 question的 语言结构  compositional linguistic structure</strong> 。建立和学习<strong>neural module networks</strong>，该过程将联合训练 jointly-trained 的模块 组成了用于回答问题的深层网络。 我们的方法<strong>将问题分解为它们的语言子结构 linguistic substructures，并使用这些结构动态实例化 模块化的网络 dynamically instan-tiate modular networks</strong>（具有可重复使用的组件， recognizing dogs, classifying colors 等 ）。由模块组合成的复合网络 are jointly trained。我们在两个具有挑战性的数据集上评估我们的方法，在 VQA natural图像数据集和有关抽象形状的复杂问题的新数据集上均获得了最新的结果</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p><strong>NMN——neural module networks</strong> 动态地基于语言结构，将模块是组成深层网络dynamically composed into deep networks based on linguistic structure.</p>
<p><strong>VQA</strong>这些问题需要计算机结合常识及视觉，语言的理解作出回答。</p>
<p><strong>recent work： </strong> </p>
<ol>
<li>将问题表示为词袋，或使用递归神经网络对问题进行编码[A neural-based approach to answering questions aboutimages.] 然后训练一个简单的分类器。</li>
<li>对于文本 QA[23] 和 图片 QA[27]，使用语义解析器 semantic parsers 将问题分解为逻辑表达式 logical expressions 。These logical expressions are evaluated against a purely logical representation of the world, which may be provided directly or extracted from an image [21]</li>
</ol>
<p><strong>并不是像传统的神经网络模型一样用一个整体，我们的方法是用多个模块化网络组合一个网络模型，模块是  specialized 和 jointly-learned。</strong> 不使用逻辑表达，<strong>我们模型输出的表达  remain entirely in the domain of visual features and attentions.</strong></p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200724165527393.png" alt="image-20200724165527393"></p>
<p><strong>模型简要：</strong></p>
<ol>
<li>使用自然语言解析器解析每个问题，以此分析出回答问题所需要的基础组成单元（attention， classification 等）以及组成单元之间的联系。</li>
<li>以上图为例，首先产生一个对狗的attention，将其输出到位置描述器  location describer。</li>
<li>根据模型具体结构，模块间传递的这些消息可能是原始图像特征、attention或分类决策 classification decision；</li>
<li>每个模块将特定输入映射到输出类型。图中用不通的颜色表示不同的模块，用绿色表示attetion-producing模块(like dog)，用蓝色表示标签模块 labeling modules (like where)。需要注意的是，所有NMN中的模块都是独立的，可组合的，这使得NMN可以对每个问题实例组合成不同的网络，在测试中出现的结构，训练过程中可能并不曾出现过，NMN也能work。</li>
<li>除NMN以外，用LSTM模块去读取question，用来学习常识性的知识。</li>
<li>additional step which has been shown to be importantfor <strong>modeling common sense</strong> knowledge and <strong>dataset biases</strong>[28]</li>
</ol>
<p><strong>evaluation:</strong></p>
<ol>
<li><strong>VQA:  </strong> VQA数据集中的许多问题都非常简单，几乎不需要 composition or reasoning。</li>
<li>一个新的合成图像数据集，其中包含涉及空间关系，集合理论推理以及形状和属性识别的复杂问题。——SHAPES——This dataset consists of complex questions about simple arrangements of colored shapes ， 问题包含两个到四个属性，对象类型或关系。SHAPES数据集包含244个唯一问题，每个问题与64个不同的图像配对（总共15616个唯一问题/图像对，训练集中有14592个，测试集中有1024个）。</li>
</ol>
<p><strong>expension: </strong></p>
<p>尽管本文考虑的所有应用程序都涉及视觉问题解答，但该体系结构更为通用，可以轻松应用于视觉参照表达解析 visual referring expression resolution[9,34]或有关自然语言文本的问题 natural language texts 解答</p>
<h3 id="2-Motivations"><a href="#2-Motivations" class="headerlink" title="2. Motivations"></a>2. Motivations</h3><ol>
<li>There is no single “best network” for full range of computer vision tasks</li>
<li>用 a prefix of a network trained for classification 作为系统的初始状态是现在的常态，大幅减少训练时间并提高准确性</li>
</ol>
<p>因此，尽管网络结构不是通用的（就同一网络而言，它适用于所有问题），但它们至少在经验上是模块化的（就一项任务而言，中间表示对于许多其他任务都是有用的）</p>
<p>语言的组成性质意味着此类步骤的处理数量可能不受限制，从问题和图像到答案的回答过程看成  highly-multitask learning setting。<strong>Moreover, multiple kinds f processing might be required—repeated convolutions might identify a truck, but some kind of recurrent architecture is likely necessary to count up to arbitrary numbers.</strong></p>
<p>所以构建 modular, composable, jointly-trained neural networks.</p>
<h3 id="3-Related-work"><a href="#3-Related-work" class="headerlink" title="3. Related work"></a>3. Related work</h3><h5 id="1-Visual-Question-Answering"><a href="#1-Visual-Question-Answering" class="headerlink" title="1. Visual Question Answering"></a>1. Visual Question Answering</h5><p>回答有关图像的问题有时称为“视觉图灵测试”，在由配对图像，问题和答案组成的合适的数据集出现之后变得热门。</p>
<p> <strong>DAQUAR dataset</strong> 仅限于室内场景 indoor scenes，并且包含的示例相对较少。</p>
<p><strong>COCOQA数据集 </strong> 和  <strong>VQA数据集</strong>  明显更大，并且具有更多的视觉多样性 visual variety。两者都是基于来自<strong>COCO dataset</strong> 的图像。尽管 COCOQA 包含从与 COCO 数据集相关联的描述 descriptions 中自动生成的问题-答案对，但 has crowed sourced questions-answer pairs 。我们评估了VQA的方法，这是两个数据集中更大，更自然的一个。</p>
<p><strong>“经典”方法包括[27,21]</strong>。这两种方法在<strong>使用 semantic parser</strong> 上类似于我们的方法，但是它们<strong>依赖于固定的逻辑推理器 logical inference</strong> ，而<strong>不是学习的合成操作  compositional operations</strong> 。</p>
<p>文献[33,26,10]中已经提出了<strong>几种用于 视觉询问 visual questioning  的神经模型</strong>，所有这些模型都使用<strong>标准的深层序列建模机制 deep sequence modeling machinery </strong>来<strong>构造图像和文本的联合嵌入</strong>，并<strong>立即将其映射 a distribution over answer</strong> 。在这里，我们<strong>试图更显式地对产生每个答案所需的计算过程进行建模</strong>，但是受益于产生序列和图像嵌入的技术，这些技术在先前的工作中已经变得很重要。</p>
<p><strong>视觉质询 visual questioning 的一个重要组成部分是在图像中定位问题</strong>。[18,32,17,20,14]中完成了 <strong>grounding task 这种基础任务</strong>，在此作者试图在图像中的定位短语 localize phrases in an image[39]，使用<strong>注意力机制预测句子生成过程中每个单词的热图 predict a heatmap for each word during sentence generation</strong>。这些方法启发了我们模型的<strong>注意力成分</strong>。</p>
<h5 id="2-General-compositional-semantics"><a href="#2-General-compositional-semantics" class="headerlink" title="2. General compositional semantics"></a>2. General compositional semantics</h5><p>在学习 <strong>如何回答 question–answer pairs 中有关结构化知识表示的问题时，有大量文献，无论有没有联合学习简单谓词的意义  meanings  [23,21]</strong>。 <strong>回答问题的 task 之外</strong>，已经提出了几种用于<strong>指令跟随的模型</strong>，这些<strong>模型在底层连续控制信号上施加了离散的“planning structure”  [</strong>1,30]。我们没有意识到过去<strong>使用 语义解析器 来预测网络结构</strong>，或<strong>更普遍地是利用 集理论 set-theoretic approaches  之于 经典语义解析方法 classical semantic parsing 和 注意方法attentional approaches 之于计算机视觉 之间的自然相似性。</strong></p>
<h5 id="3-Neural-network-architectures"><a href="#3-Neural-network-architectures" class="headerlink" title="3. Neural network architectures"></a>3. Neural network architectures</h5><p>为每个输入数据选择不同的网络图的想法是<strong>递归网络（其中网络随着输入长度的增长而增长）</strong>和<strong>递归神经网络（例如根据句法结构构建网络）</strong>的基础（36）。但是，这两种方法最终都涉及到单个计算模块（例如LSTM [13]或GRU [5]单元）的重复应用。从另一个方向看，某些类型的存储网络[38]可以看做是我们模型的一种特殊情况，带有固定的计算图，由一列   <strong><em>find</em></strong>  modules  和 一个 describe module  组成。具有模块化子结构的其他 policy- and algorithm-learning approaches 包括[16,4]。[31]描述了一种程序，该程序用于学习从行为被完全指定的  functional primitives 集合中汇编程序的过程。</p>
<p><strong>主要的贡献： </strong>  动态组装模块，同时允许节点执行不同类型的“消息”（原始图像特征，关注度和分类预测）来进行异构计算并在模块间传递“消息”</p>
<h3 id="4-Neural-module-networks-for-visual-QA"><a href="#4-Neural-module-networks-for-visual-QA" class="headerlink" title="4. Neural module networks for visual QA"></a>4. Neural module networks for visual QA</h3><p>每个训练的数据项被想象成一个三元组$(w,x,y)$:</p>
<ul>
<li>$w$  is a natural-language question</li>
<li>$x$  is an image</li>
<li>$y$  is an answer</li>
</ul>
<p>整个模型由<strong>一个模块集 $\{m\}$</strong> 完全指定，<strong>相关参数为 $\theta_m$，</strong>以及  <strong>a network layout predictor</strong>   $P$  ，它<strong>将字符串映射成网络</strong>。给定 $(w,x)$ ，通过 $P(w)$ 实例化一个网络。并使用 $x$（或 $w$ ）作为输入，得到一个关于标签的分布( (for the VQA task, we require the output module produce an answer representation))。所以预测分布  predictive distribution 可以表示为 $p(y|w, x; \theta)$</p>
<h4 id="4-1-Modules"><a href="#4-1-Modules" class="headerlink" title="4.1. Modules"></a>4.1. Modules</h4><p>我们的目标是确定可以组装为任务所需的所有配置的<strong>一小部分模块  modules  </strong>。这<strong>对应于识别可组合的视觉原语  composable vision primitives 的最小集合</strong>。这些模块操作<strong>三种基本数据类型：图像， unnormalized attentions 和 标签 labels</strong>。对于本文描述的特定任务和模块，<strong>几乎所有有趣的组合现象都发生在 注意力空间 the space of attentions 中</strong>，并且将我们的贡献更狭义地描述为“注意力组合”网络并非没有道理。但是，将来可能会轻松添加其他类型（用于新应用或在VQA域中具有更大的覆盖范围）    </p>
<p><strong>Notion:</strong>  $\text{TYPE<a href="ARG_1...">INSTANCE</a>}$</p>
<p><strong>example:  </strong> $\text{}$ $\text{find[red]}$ locates red things;      $\text{find[dog]}$  locates dogs</p>
<p><strong>Weights may be shared at both the type and instance level</strong></p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725094320268.png" alt="image-20200725094320268"></p>
<p>$\text{find[c]}$  ：对图像卷积 with a weight vector ,不同 c 是不同的 weight vector，生成 a heatmap or unnormalized attention.</p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725101355203.png" alt="image-20200725101355203"></p>
<p>$\text{transform[c]}$ ：多层感知机实现， performing a fully-connected mapping from one attention to another。不同 c 是不同的 mapping  weight。<strong>作用：</strong> <strong>take an attention and shift the regions of greatest activation upward</strong> 。<strong>$\text{transform[not]}$  should move attention awayfrom the active regions</strong>  实验经验说明，第一个FC层输出向量大小为32，第二个FC输出和transform的输入维度一致</p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725102011503.png" alt="image-20200725102011503"></p>
<p>$\text{combine[c]}$ ： <strong>作用  merges two attentions into a single attention</strong>   $\text{combine[and]}$ ： be active only in the regions that are active <strong>in both inputs</strong>     $\text{combine[or]}$ ： be active where the <strong>first input isactive and the second is inactive</strong>    </p>
<p><strong>transform 和 combine 对应的 问题</strong> —— 识别shape 和 color ，以及 其中的 空间上和 逻辑上的联系，需要转移 和 组合 attention</p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725140429705.png" alt="image-20200725140429705"></p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725102258978.png" alt="image-20200725102258978"></p>
<p>$\text{describe[c]}$  输入：an attention and the input image 。 将两者映射到关于 label 的分布。<strong>过程： </strong> <strong>first computes an average over image features weighted by the attention；  then passes this averaged feature vector through a single fully-connected layer</strong>  例如，describe [color]应该返回所关注区域中颜色的表示</p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725102646051.png" alt="image-20200725102646051"></p>
<p>$\text{measure[c]}$  <strong>takes an attention alone and maps it to a distribution over label</strong>  由于模块之间传递的 attention 是没经过 normalization 的， 所以经过 <strong>$\text{measure}$ 模块可以  evaluating the existence of a detected object, or counting sets of objects</strong></p>
<h4 id="4-2-From-strings-to-networks"><a href="#4-2-From-strings-to-networks" class="headerlink" title="4.2. From strings to networks"></a>4.2. From strings to networks</h4><p>已经建立了模块集合，就需要将它们根据不同问题组装成不同的网络布局。从自然语言问题到神经网络实例化有两个步骤。</p>
<ol>
<li>我们 将自然语言问题 映射 到 布局 layouts，布局包括所用模块和模块间的连接关系；</li>
<li>根据 layouts to assemble the final prediction networks.</li>
</ol>
<p>对<strong>自然语言问题</strong>：use <strong>standard tools pretrained on existing linguistic resources</strong> to obtain <strong>structured representations of questions</strong></p>
<p>之后对这一块的修改，可以是将 question 的结构化表示预测 可以和 后面的 jointly learn</p>
<h4 id="Parsing"><a href="#Parsing" class="headerlink" title="Parsing"></a>Parsing</h4><p>作者使用 <a target="_blank" rel="noopener" href="http://nlp.stanford.edu:8080/parser/">Stanford Parser</a> 对每个问题进行解析，<strong>依存句法分析</strong> 表达了句子各部分之间（例如，<strong>对象及其属性或事件及其参与者之间的语法关系</strong>），并提供了一种轻量级的抽象，使其脱离了句子的表面形式。解析器还执行基本的词法处理 lemmatization,，例如将 kites 转成  kite 和  were  转成   be。这减少了模块实例的稀疏性——标准化减少空间。</p>
<p>接下来，我们<strong>过滤出</strong> 与<strong>问题中</strong>的 <strong>wh词疑问词</strong>  和 <strong>copula系动词</strong> 相关联的 <strong>依存关系集</strong>（遍历的确切距离 根据任务的不同 而变化 以及 $how many$ 是作为特殊情况对待 ）。这给出了一个简单的符号形式来表达句子含义的（主要）部分。比如， what is standing in the field 变成了 what(stand)； what color is the truck 变成了 color(truck)；is there a circle next to a square 变成了is(circle, next-to(square))。</p>
<ul>
<li>这些表示与组合逻辑  combinatory logic  [23]有一些相似之处：每个<strong>叶节点都是隐含的以图像为输入的函数，而根表示计算的最终值</strong>。但是，尽管我们的方法是compositional and combinatorial 的，但不是 logical 的： 推论计算对神经网络产生的连续表示进行操作，仅在最终答案的预测中变得离散（啥？）</li>
</ul>
<h4 id="Layout"><a href="#Layout" class="headerlink" title="Layout"></a>Layout</h4><p><strong>modules 的确定 取决于 the structure of the parse</strong></p>
<p><strong>所有的 leaves 都是 $\text{find}$ modules， 所有中间节点都是 $\text{transform}$  或是  $\text{combine}$ modules， dependent on their arity，所有 root 节点 是  $\text{describe}$  或是  $\text{measure}$  depending on the domain</strong> </p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725112729958.png" alt="image-20200725112729958" style="zoom:80%;" />网络结构相同的，在同一个 batch 中处理</p>
<p>此转换过程的某些部分是特定于任务的，我们发现相对简单的表达式最适合自然图像问题 natural image questions，而合成数据（通过设计）则需要更深层次的结构</p>
<h4 id="Generalizations"><a href="#Generalizations" class="headerlink" title="Generalizations"></a>Generalizations</h4><p>输入到 layout 的 input 可以是 natural language parser出来的 dependencies ，也可以是 SQL-like queries </p>
<h4 id="4-3-Answering-natural-language-questions"><a href="#4-3-Answering-natural-language-questions" class="headerlink" title="4.3. Answering natural language questions"></a>4.3. Answering natural language questions</h4><p>我们的最终模型将<strong>神经模块网络的输出</strong> 与 simple <strong>LSTM question encoder 的预测  prediction</strong> 相结合。(提供问题的完整信息)</p>
<ol>
<li>因为 <strong>Parsing</strong> 后的简单表示   what <strong>is</strong> standing in the field 变成了 what(stand)  不会实质性地改变问题的语义的语法提示被丢弃。 The question encoder thus allows us to model underlying <strong>syntactic regularities 句法规则</strong> in the data。</li>
<li>allows us to capture <strong>semantic regularities</strong> 语义规律  it is reasonable to guess that <em>what color is the bear?</em>    is answered by <strong>brown</strong> ，unreasonable to guess <strong>green</strong></li>
</ol>
<p>为了计算 answer，我们<strong>将LSTM的 final hidden state 通过一个 FC</strong>，将其 逐元素 添加到 NMN 的根模块生成的表示中，应用ReLU非线性，然后另一个 FC 层， 一个 softmax  obtain a distribution over answers.</p>
<p>为了与以前的工作保持一致，我们将答案预测视为一个纯粹的分类问题：<strong>该模型从训练过程中观察到的答案中选择（无论它们是否包含多个单词）</strong>，并将<strong>每个答案视为不同的类别</strong>。因此，在该最终预测层中的，例如 <em>left side</em> 和 <em>left</em>  之间没有共享参数。通过循环解码器一次生成一个单词得到的多单词答案的模型可以作为 extension</p>
<h3 id="5-Training-neural-module-networks"><a href="#5-Training-neural-module-networks" class="headerlink" title="5. Training neural module networks"></a>5. Training neural module networks</h3><p><strong>objective：  </strong>  find module parameters maximizing the likelihood of the data——softmax 结果</p>
<p>最后一层都是设计成 输出 在 label 上的概率分布，  so each assembled network also represents a probability distribution.  (视为分类问题)</p>
<p>由于用于回答问题的动态网络结构，某些权重比其他权重更频繁地更新。因此，我们发<strong>现具有单个权重自适应学习率 adaptive per-weight learning rates  的学习算法</strong>在性能上要比简单的梯度下降好得多。下面描述的所有实验<strong>均使用具有标准参数设置的 ADADELTA</strong></p>
<h3 id="6-Experiments-compositionality"><a href="#6-Experiments-compositionality" class="headerlink" title="6. Experiments: compositionality"></a>6. Experiments: compositionality</h3><p>这项工作的主要目标之一是学习用于深层语义组合的模型 deep semantic compositionality。 To eliminate mode-guessing as a viable strategy, all questions have a yes-or-no answer, but good performance requires that the system learn to recognize shapes and colors, and understand both spatial and logical relations among sets of objects.——为了消除模式猜测作为一种可行的策略，所有问题都回答是或否，但是良好的性能要求系统学会识别形状和颜色，并理解各组图形之间的空间关系和逻辑关系对象</p>
<p>为了产生一组初始的图像特征，我们将输入图像通过LeNet [22]的卷积部分，这个卷积部分与模型的  question-answering 部分共同训练。我们比较了我们的方法 与 以类似于[33]所述的方法 重新实现的 VIS + LSTM 基线，再次用LeNet替换了预先训练的图像嵌入。</p>
<p>此外，颜色检测器和注意力转换的行为符合预期（图2b），表明我们的联合训练模型正确地在模块之间分配了责任。这证实了我们的方法能够对复杂的组成现象进行建模，而先前的方法无法解决视觉问题。</p>
<h3 id="7-Experiments-natural-images"><a href="#7-Experiments-natural-images" class="headerlink" title="7. Experiments: natural images"></a>7. Experiments: natural images</h3><p>处理涉及自然图像的硬性感知问题 hard perceptual problems </p>
<p>we <strong>evaluate on the VQA dataset</strong>  ，这是同类资源中最大的一种，由来自MSCOCO的200,000张图像组成[25]，每张图像都由人类注释者生成的三个问题和每个问题十个答案配对。我们使用标准训练/测试集划分 ，仅训练那些 answer 标记 为高置信度的模型。</p>
<p> <strong>The visual input to the NMN</strong>   is <strong>the conv5 layer of a 16-layer VGGNet</strong> [35] after max-pooling, with features normalized to have mean 0 and standard deviation 1.  出来 在 ImageNet 上预训练的 VGG，还比较了在  MSCOCO  上  fine-tuned 过的 VGG。</p>
<p>我们发现，即使始终在问题上涉及 量化 quantification，顶层模块 总是 <strong>describe</strong>，才能最好地完成此任务。</p>
<p><img src="https://raw.githubusercontent.com/JasonGuojz/JasonGuojz.github.io/master/images/Neural Module Networks/image-20200725144242301.png" alt="image-20200725144242301" style="zoom:80%;" /></p>
<p>compare to ：</p>
<ol>
<li>a text-only baseline (LSTM), </li>
<li>a pre-vious baseline approach that predicts answers directly from an encoding of the image and the question [3]</li>
<li>an at-tentional baseline (ATT+LSTM). This last baseline shares the basic computational structure of our model without syntactic compositionality: it uses the same network layout for every question (a <strong>find</strong> module followed by a <strong>describe</strong> module), with parameters tied across all problem instances</li>
</ol>
<p>稀有 rare 单词（在训练数据中出现的次数少于10次）被映射到LSTM编码器和模块网络中的单个 tolen 或 模块实例。</p>
<p>对解析器输出的调查还表明，使用更好的解析器还有很大的空间可以改进系统性能。</p>
<p><strong>通过手动检查发现： more complicated questions are more prone to picking up irrelevant predicates. 问题一复杂，可能找到不相关的谓词 。For example $\text{are these people most likely experiencing a workday?}$   is parsed as  $\text{be(people,  likely)}$   when the desired analysis is   $\text{be(people,  work)}$.   Parser errors of this kind could be fixed with joint learning. 即 parse过程和后面的 module 选择等一起learn。</strong></p>
<p><strong>系统做出的谓词误判，包括可能的语义混淆（将纸板解释为皮革，将圆形窗框解释为时钟）(cardboard interpreted as leather, round windows interpreted as clocks)、 正常的词汇变化lexical variation（<em>container</em> for <em>cup</em>），以及使用优先级高但与图片无关的答案（describing a horse as located in a pen rather than a barn）。</strong></p>
<h3 id="8-Conclusions-and-future-work"><a href="#8-Conclusions-and-future-work" class="headerlink" title="8. Conclusions and future work"></a>8. Conclusions and future work</h3><p>到目前为止，我们在预测网络结构 (dependency parse) 和学习网络参数之间保持了严格的区分。It is easy to imagine that these two problems might be solved jointly，但在整个训练和解码过程中，网络结构的不确定性仍然存在。这可以通过使用某些高级机制“参与”计算的相关部分，或者通过与现有的用于学习语义分析器learning semantic parsers的工具集成，来通过单片网络 monolithic network 来完成。<strong>我们在这项工作的后续工作中描述了联合学习模块行为和解析器的第一步[2]</strong>。</p>
<p>事实上，我们的神经模块网络可以训练以产生可预测的输出（即使自由组合），这一事实指向了更通用的“程序”范式。由神经网络构建而成。在这种范式中，网络设计人员（人工或自动化）可以访问神经零件的标准套件，从中构造用于执行复杂推理任务的模块。虽然视觉问题回答为该方法提供了自然的测试平台，但它的用途可能更广泛，可扩展到有关文档和结构化知识库的查询，或者扩展到更通用的函数逼近和信号处理。46</p>
<h4 id="reference1"><a href="#reference1" class="headerlink" title="reference1"></a><a target="_blank" rel="noopener" href="https://jimlee4530.github.io/Neural Module Networks实验笔记及总结">reference1</a></h4><h4 id="reference2"><a href="#reference2" class="headerlink" title="reference2"></a><a target="_blank" rel="noopener" href="https://bair.berkeley.edu/blog/2017/06/20/learning-to-reason-with-neural-module-networks/">reference2</a></h4><h4 id="reference3"><a href="#reference3" class="headerlink" title="reference3"></a><a target="_blank" rel="noopener" href="http://ronghanghu.com/n2nmn/">reference3</a></h4>
    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>guo jiazhen
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://jasonguojz.github.io/blog/2020/07/25/Neural%20Module%20Networks/" title="Neural Module Networks">https://jasonguojz.github.io/blog/2020/07/25/Neural Module Networks/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/blog/tags/dependency-parsing/" rel="tag"># dependency parsing</a>
              <a href="/blog/tags/CLEVR-dataset/" rel="tag"># CLEVR dataset</a>
              <a href="/blog/tags/SHAPES-dataset/" rel="tag"># SHAPES dataset</a>
              <a href="/blog/tags/dynamic-network-structure/" rel="tag"># dynamic network structure</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/blog/2020/07/23/DL2%20TRAINING%20AND%20QUERYING%20NEURAL%20NETWORKS%20WITH%20LOGIC/" rel="prev" title="DL2- TRAINING AND QUERYING NEURAL NETWORKS WITH LOGIC">
      <i class="fa fa-chevron-left"></i> DL2- TRAINING AND QUERYING NEURAL NETWORKS WITH LOGIC
    </a></div>
      <div class="post-nav-item">
    <a href="/blog/2020/07/26/The%20Neuro-Symbolic%20Concept%20Learner%20Interpreting%20Scenes%20Words%20and%20Sentences%20From%20Natural%20Supervision/" rel="next" title="The Neuro-Symbolic Concept Learner Interpreting Scenes Words and Sentences From Natural Supervision">
      The Neuro-Symbolic Concept Learner Interpreting Scenes Words and Sentences From Natural Supervision <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E3%80%8ENeural-Module-Networks%E3%80%8F%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0"><span class="nav-number">1.</span> <span class="nav-text">『Neural Module Networks』阅读笔记</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#code"><span class="nav-number">2.</span> <span class="nav-text">code</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%93%E6%9C%89%E5%90%8D%E8%AF%8D"><span class="nav-number">3.</span> <span class="nav-text">专有名词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract"><span class="nav-number">4.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Introduction"><span class="nav-number">5.</span> <span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Motivations"><span class="nav-number">6.</span> <span class="nav-text">2. Motivations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Related-work"><span class="nav-number">7.</span> <span class="nav-text">3. Related work</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-Visual-Question-Answering"><span class="nav-number">7.0.1.</span> <span class="nav-text">1. Visual Question Answering</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-General-compositional-semantics"><span class="nav-number">7.0.2.</span> <span class="nav-text">2. General compositional semantics</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-Neural-network-architectures"><span class="nav-number">7.0.3.</span> <span class="nav-text">3. Neural network architectures</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Neural-module-networks-for-visual-QA"><span class="nav-number">8.</span> <span class="nav-text">4. Neural module networks for visual QA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-Modules"><span class="nav-number">8.1.</span> <span class="nav-text">4.1. Modules</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-From-strings-to-networks"><span class="nav-number">8.2.</span> <span class="nav-text">4.2. From strings to networks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Parsing"><span class="nav-number">8.3.</span> <span class="nav-text">Parsing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Layout"><span class="nav-number">8.4.</span> <span class="nav-text">Layout</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Generalizations"><span class="nav-number">8.5.</span> <span class="nav-text">Generalizations</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-Answering-natural-language-questions"><span class="nav-number">8.6.</span> <span class="nav-text">4.3. Answering natural language questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Training-neural-module-networks"><span class="nav-number">9.</span> <span class="nav-text">5. Training neural module networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Experiments-compositionality"><span class="nav-number">10.</span> <span class="nav-text">6. Experiments: compositionality</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-Experiments-natural-images"><span class="nav-number">11.</span> <span class="nav-text">7. Experiments: natural images</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-Conclusions-and-future-work"><span class="nav-number">12.</span> <span class="nav-text">8. Conclusions and future work</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#reference1"><span class="nav-number">12.1.</span> <span class="nav-text">reference1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#reference2"><span class="nav-number">12.2.</span> <span class="nav-text">reference2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#reference3"><span class="nav-number">12.3.</span> <span class="nav-text">reference3</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description">谦卑</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/blog/archives/">
        
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/blog/categories/">
          
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/blog/tags/">
          
        <span class="site-state-item-count">79</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/JasonGuojz" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;JasonGuojz" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/blog/jzguovulcan@gmail.com" title="E-Mail → jzguovulcan@gmail.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="skype:1142346305?call|chat" title="Skype → skype:1142346305?call|chat" rel="noopener" target="_blank"><i class="fa fa-fw fa-skype"></i>Skype</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder"></span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="Symbols count total">195k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">2:58</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/blog/lib/anime.min.js"></script>
  <script src="/blog/lib/velocity/velocity.min.js"></script>
  <script src="/blog/lib/velocity/velocity.ui.min.js"></script>

<script src="/blog/js/utils.js"></script>

<script src="/blog/js/motion.js"></script>


<script src="/blog/js/schemes/muse.js"></script>


<script src="/blog/js/next-boot.js"></script>




  




  
<script src="/blog/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '2M2aiiBff85KzN3JvTFsCV2J-9Nh9j0Va',
      appKey     : '7n1fFOWyaLnTBNpNjRhYsF0W',
      placeholder: "Please write your comments here",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
